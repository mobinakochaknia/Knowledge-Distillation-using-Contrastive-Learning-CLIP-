# Knowledge-Distillation-using-Contrastive-Learning-CLIP-
This project explores knowledge distillation from a large monolingual model to a smaller multilingual model using contrastive learning. Specifically, we employ CLIP loss to transfer knowledge effectively.
